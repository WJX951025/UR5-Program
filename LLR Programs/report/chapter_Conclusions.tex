\chapter{Conclusions}\label{chap:Conclusions}

\ac{RL} covers a wide variety of methods, ranging from off-line model-based to on-line model-free methods. In this work we focused on model-learning methods that build a model of the environment during the learning process. The implemented algorithms are Dyna-style methods, which interact with the model and the actual system simultaneously. The research was divided into two parts. First we investigated how a state-transition model could be built during learning. Thereafter we added the model to the learning process and investigated how the model could be implemented, such that the learning process benefits optimally from the state-transitions generated by the model.




\section{Modeling}
We used the memory-based \ac{LLR} method to build a model. The memory consists of the observed state-transitions and can be updated continuously during the learning process. Memory-based methods have advantages that are useful in \ac{RL}. It is very easy to improve the model during learning, as this simply means adding new experiences to the memory. Furthermore, results from statistical analysis can be used to assess the quality of a particular estimated state-transition. Two limitations of memory-based modeling are noise sensitivity and computational effort needed for a query. The first is counteracted by using linear regression, which estimates a linear function through the set of nearest neighbors. The computational effort is decreased by storing the memory samples using a $k$d-tree.

We showed that the \ac{LLR} model is able to estimate state-transitions from relatively few observations. \ac{LLR} was able to model the walking motion of a complex, humanoid robot setup using a memory of a few thousand samples. This shows that also high-dimensional setups can be estimated using \ac{LLR}.

For use in a \ac{RL} setting, it is desired to have a measure of the quality of the estimated state-transition. This measure should not be based on the real value, as this might not be available in a \ac{RL} setting. We proposed the statistical notion of prediction intervals as a possible measure for the model accuracy. This measure is based on the residuals of the estimated model and only depends on the memory samples used in the linear regression.




\section{Reinforcement Learning}
We performed learning experiments on an inverted pendulum simulation that learned a swing-up task. The learning experiments were performed using model-learning algorithms, combining interaction with the real environment and the built model. We used \ac{LLR} as a model, using the observed state-transitions as memory samples. 

We showed that Dyna-style learning increases the learning speed, compared to model-free SARSA. However, the final performance of the Dyna algorithm is slightly worse than SARSA. This is probably due to the inaccurate state-transitions introduced by the model. For Dyna with randomly selected model-based updates, using many uncertain state-transitions resulted in faster learning than using few accurate state-transitions. For random Dyna, ignoring the prediction intervals made learning faster.

To generate specific model-based state-transitions instead of randomly chosen state-action pairs as in Dyna, two methods were introduced. Both \ac{PS} and \ac{LA Dyna} led to faster learning than random Dyna. As opposed to random Dyna, these methods needed a limit on the prediction interval. Without such a limit, severe 'unlearning' occurred and convergence to an optimal trajectory towards the goal was not always achieved. However, choosing this limit too tight resulted in slow learning. Out of a set of four compared prediction interval limits, the optimal value for \ac{PS} and \ac{LA Dyna} resulted in discarding 7\% and 4\% of the estimated state-transitions repectively. 



\section{Discussion and future work}
In Chapter \ref{chap:Modeling} we suggested local optimization of the number of nearest neighbors and outlier detection as methods to improve the quality of the model locally. In our learning experiments these methods were not used, primarily because they consume extra computation time. However, local optimization of $K$ might be usable if sufficient computational power is available and outlier detection might be needed when \ac{LLR} is used on real setups.

If a model is used to generate state-transitions, the model should be accurate enough to not disturb the learning process. In this work we have used the statistical notion of prediction intervals to assess the quality of the modeled state-transitions. We showed that in the case of \ac{PS} and \ac{LA Dyna} it is indeed needed to limit the prediction interval. In random Dyna this was not needed. Further research is needed to better understand how the quality of the model affects the learning process. 

We have briefly addressed the question how the limit of the prediction interval should be chosen. We have connected the prediction interval limit to the discriminating power of the value function approximator. It is not fully understood how the quality of the model and the resolution of the function approximator relate to each other. It might be true that a very accurate model is not needed if the resolution of the function approximator is relatively coarse.

The \ac{RL} experiments in Chapter \ref{chap:Prioritized Sweeping} were performed using an inverted pendulum simulation. The \ac{LLR} model and the different algorithms were all applied in this experimental setting. For future research, more complex setups and real-time applications could be considered. Such an application could be the humanoid robot setup (see Section \ref{sec:LLR-robot leo}). The modeling results with this setup indicate that \ac{LLR} is able to model such a system. Furthermore, all algorithms should be fast enough to be used in real-time applications, given that the sampling frequency is not too high. Therefore, learning experiments with the humanoid robot setup could also benefit from the methods introduced in this work.

We showed that \ac{PS} and \ac{LA Dyna} learn faster than Dyna with randomly selected model-based updates. \ac{PS} and \ac{LA Dyna} use different methods for selecting state-transitions. \ac{PS} looks backwards, while \ac{LA Dyna} looks ahead from the current state. In order to fully understand why these methods work and whether this is restricted to certain learning problems, further research is needed. Also different methods for selecting modeled state-transitions could be compared.

One of the major issues with \ac{PS} is how to determine lead-in states. Our method calculates a lead-in state for every available action separately. This method becomes infeasible with a large or continuous action-space. It is not clear how lead-in states could be determined more easily.

\section{Final conclusions}

In the introduction of this thesis we formulated the research question, which can now be answered. The memory-based modeling method \ac{LLR} showed very good modeling capabilities and can be implemented easily in a \ac{RL} algorithm. \ac{LLR} was able to model a complex humanoid robot setup surprisingly accurate using only a few thousand memory samples. The model can be used to generate state-transitions that can be used by the learning algorithm as if they were real observations. The learning speed is influenced by the position in state-space where the state-transitions are generated. \ac{PS} and \ac{LA Dyna} try to concentrate these state-transitions in the neighborhood of the trajectory towards the goal, which resulted in faster learning. 

Our learning experiments were performed on a relatively simple system in simulation. However we are confident that the methods described in this thesis can also be used on more complex, real setups. The modeling capability of \ac{LLR} and the computational speed of the algorithms are both sufficient to be applied to such systems.

